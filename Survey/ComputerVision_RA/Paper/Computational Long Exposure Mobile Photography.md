## Computational Long Exposure Mobile Photography

#### 1 INTRODUCTION

Mobile photography is ever present in consumersâ€™ daily lives and is often superseding traditional photography. Using image burst capture and post-processing techniques, modern mobile phonesâ€™ imaging pipelines produce very high quality results, providing high dynamic range tone mapping, exceptional low light performance and simulating depth-of-field bokeh effects, which were previously achievable only with much bigger and heavier cameras and lenses. 

Despite these outstanding improvements, long exposure mobile photography remains poorly treated to the best of our knowledge. Existing solutions donâ€™t help users produce results where moving and static scene elements appear blurry and sharp respectively. This juxtaposition of sharp against blurry is a key property of a compelling image, that cannot be achieved by simply exposing a hand-held camera sensor for a longer duration.

Traditional long exposure photography is typically performed in one of two ways, according to the scene and situation. One approach produces a foreground blur effect (e.g. silky waterfall, light trails, etc.) over a sharp background, using very long exposure times that can last up to several seconds. This requires using a tripod, as even a slight camera shake can cause undesired loss of background sharpness. Additionally, a neutral density (ND) filter must be added to the lens, to avoid over-exposing the sensor. A second approach, called panning photography, produces a rendition with a sharp moving subject over a background that is blurred with motion relative to the subject. It is achieved by tracking the moving subject with the camera, while keeping the shutter open with the exposure time increased modestly, e.g. half a second, and the aperture slightly reduced to avoid over-exposing the image. The photographer must track the subject motion as precisely as possible to avoid undesired loss of subject sharpness, while also pressing the shutter button at the right moment. Both approaches require advanced skills, practice and choosing the camera shutter speed manually, taking into account how fast the scene is moving to achieve the desired result. 

The main contribution of this paper is a computational long exposure mobile photography system, implemented in two variants, which democratize the two aforementioned use cases. It is implemented in a new camera mode called "Motion Mode" on Google Pixel 6 and 7 smartphones, which allows the user to easily capture these effects, without the need for a tripod or lens filter, nor the need to track the moving subject precisely or at all. Our method is fully automatic end-to-end within each variant: after the user chooses which of foreground or background blur result they wish to produce, we generate long exposure 12 megapixel photographs at the tap of the shutter button, while compensating for camera and/or subject motion, thereby preserving desired background and subject sharpness. The main components of our system are:

-  Capture schedule and frame selection, producing normalized blur trail lengths independent of scene or camera velocity, 
- Subject detection that combines gaze saliency with people and pets face region predictions, and tracking of their motion, 
- Alignment of input images to cancel camera hand-shake, stabilize the background in the presence of moving foreground elements, or to annul subject motion while producing pleasing background motion blur trails, 
- Dense motion prediction and blur synthesis, spanning multiple high resolution input frames and producing smooth curved motion blur trails with highlight preservation.

Furthermore, our system architecture, which includes several neural networks, performs efficiently on a mobile device under constrained compute and memory budgets, implementing an HDR imaging pipeline that produces both related conventional and long exposure results in just a few seconds. 

#### 2 RELATED WORK 

##### 2.1 Mobile Phone Computational Photography 

Many computational photography advances in recent years define todayâ€™s mobile photography capabilities. The work from Hasinoff et al. [2016] describes a mobile camera pipeline that captures, aligns and merges bursts of under-exposed raw images. Combined with the work of Wronski et al. [2019], they are able to strongly improve the Signal to Noise Ratio (SNR), dynamic range and image detail, overcoming the limitations of small smartphone sensors and lenses. Our system is built on top of such a computational imaging foundation. 

To handle very low light situations without using a flash, Liba et al. [2019] employ a scene motion metering approach to adjust the number of input burst frames and determine their exposure time. Similarly, we adjust the frame capture schedule based on scene motion, estimated when the shutter button is pressed, for the purpose of normalizing the resulting amount of motion-blur. 

Since the small camera lenses used on smartphones cannot produce shallow depth-of-field effects optically, Wadhwa et al. [2018] design a synthetic bokeh approach, that relies on a semantic person segmentation neural network, with the intent to isolate a subject from a distracting background. Our system is analogous, as we strive to isolate a subject from the relative motion of the background, while attempting to emphasize the dynamic nature of the scene. 

##### 2.2 Auto-tracking a Subject (background blur) 

Determining the subject of a background blur capture is a hard problem. Many synthetic long exposure pipelines avoid it altogether by requiring manually tagging the subject region, or using a heuristic such as the middle region of the image [Lancelle et al. 2019; Luo et al. 2018; Mikamo et al. 2021]. In contrast, we present a pipeline which determines the subject region automatically by predicting visual saliency and face regions. 

Using saliency-driven image edits to highlight a main subject from a distracting background was introduced in [Aberman et al. 2022]. Existing methods to detect and track subject motion over time include [Stengel et al. 2015], which use gaze saliency prediction to detect the subject and optical flow to track its motion, and [Mikamo et al. 2021], which require the user to specify the subject region using a bounding box and similarly track its motion. In our work, we detect the subject using a combination of gaze saliency and semantic segmentation using a highly efficient mobile architecture inspired by [Bazarevsky et al. 2019]. We track its motion using feature tracking, and introduce an alignment regularization term to result in more visually pleasing motion-blur trails, which are more consistent with overall subject motion.

##### 2.3 Stabilizing the Background (foreground blur) 

Images captured by handheld cameras are often shaky and may often contain parallax. In the foreground-blur case, we need to stabilize the background to keep the resulting regions sharp. This can be solved for using structure-from-motion (SFM) techniques [Liu et al. 2014] to construct a 3-d representation of the scene [Hartley and Zisserman 2004], then a stabilized camera path can be solved for in 3-d, and the scene finally re-rendered using the new smooth path [Liu et al. 2009]. However, these techniques rely on 3-d reconstruction, for which a fast and robust implementation is challenging [Liu et al. 2011]. At the other end of the spectrum are 2-d stabilization techniques that use much simpler motion models such as global homographies or affine transformations [Morimoto and Chellappa 1998]. These techniques are fast and robust, but cannot model effects such as parallax, rollingshutter, or lens distortion. There is a large body of work that extends these 2-d methods, such as using gyroscopic sensors only [Karpenko et al. 2011], gyroscopes with face detection [Shi et al. 2019], targeted crops [Grundmann et al. 2011] and trajectory filtering [Liu et al. 2011]. Our method is analogous to techniques that start with 2-d feature trajectories to estimate per-frame global transformations and refine this estimate with spatially varying image warps [Liu et al. 2013; Zaragoza et al. 2013] to achieve the desired trade-off between speed and robustness. 

##### 2.4 Synthesizing Motion Trails 

There is a large body of prior work on synthesizing motion-blur, in the context of non-photorealistic rendering [Lee et al. 2009], stop-motion animation [Brostow and Essa 2001], or 3D computer graphics rendering in real-time [RÃ¸nnow et al. 2021] or offline [Lehtinen et al. 2011; Navarro et al. 2011]. There is work describing single photograph post-processing interactive applications to create artistic motion effects [Luo et al. 2018, 2020; Teramoto et al. 2010] or that process multiple previously stabilized images and can achieve non-physical renditions [Salamon et al. 2019]. 

Our work compares more directly to prior work on computational long exposure from multiple photographs or video frames. Lancelle et al. [2019] describe a pipeline that can handle both foreground and background blur effects, but requires substantial user interaction to handle all the cases. Like other approaches, they require significant compute time in an offline processing application, as they rely on expensive optical-flow based image warping or frame interpolation, to synthesize smooth motion-blur spanning the input frames pairwise. In contrast, our pipeline is fully automatic, is integrated in a smartphone photo camera and produces the result in a few seconds at 12 megapixel resolution. To synthesize motion-blur, we use a line kernel prediction neural network, derived from [Brooks and Barron 2019], combined with a GPU rendering algorithm that can handle the input image alignment warping implicitly, while producing smooth and curved motion-blur trails spanning multiple input frames. 

#### 3 SYSTEM OVERVIEW 

A diagram of our computational long-exposure system is shown in Figure 2. The stream of captured raw images is processed incrementally at two different resolutions through four stages, each corresponding to a row of the diagram in Figure 2: initial subject detection, motion analysis, motion prediction and rendering. The initial saliency and face prediction stage (Section 4.2) computes the main signals for our subject detection, producing a normalized weight map. The motion analysis stage is responsible for tracking (Section 4.3) and aligning (Section 4.4) a set of feature points corresponding to the detected subject or to the background, and for selecting frames based on motion statistics (Section 4.5). The motion prediction stage (Section 4.6) predicts dense line kernel and weight maps, that are used in the rendering stage (Section 4.7) to produce smooth motion-blur spanning a given input frame pair. The final compositing stage (Section 4.8) layers the final results while preserving the sharpness of important areas in the final image. 

The first three stages use as their input, images that have been tone-mapped and converted to sRGB, downsampled by a factor of 8 to a low resolution of 504 x 376. This resolution is chosen to achieve low latency when processing frames, which is dominated by the dense motion prediction neural network. This also ensures that the receptive field covers most practical motion disparities in the input full resolution images. The last stage however, uses the intentionally under-exposed burst raw images converted to 16-bit linear RGB. The high bit-depth is necessary to preserve the sceneâ€™s high dynamic range during rendering, i.e. to avoid saturating the highlights and banding artifacts in the shadows. Images are downsampled by a factor of 2 as a trade-off that preserves enough detail in the final result while operating within a reduced memory footprint. 

The incremental processing loop converts and downsamples an additional burst frame at each iteration, feeding the three last stages of our pipeline and resulting in an accumulated averaged motionblur image. The loop stops when the frame selection criteria is reached, using an estimate of motion-blur trailsâ€™ length. We then composite the final results, while upsampling images back to full resolution. At the end of our pipeline, images are converted to a low dynamic range 8-bit representation, using tone-mapping to preserve the high dynamic range visual appearance of the scene. 

#### 4 IMPLEMENTATION 

##### 4.1 Burst Capture 

Our camera system captures frames at a rate of 30 frames per second, using fully automatic aperture, shutter speed, and focus settings. These settings may adjust dynamically to scene changes across frames, and our system performs well with all but abrupt variations. 

In the background blur case, we target scenes with fast moving nearby subjects. When tapping the shutter button, the most recently captured frame is used as the base frame to produce the conventional exposure [Hasinoff et al. 2016], as it depicts the peak of the action chosen by the user. Up to 8 additional frames in the past may then be selected by our algorithm to produce the background blur effect. 

When producing a foreground blur effect, we target scenes with a much larger range of scene motion velocity, including slow and far away moving content. To produce a compelling effect, this requires extending the capture for a duration up to several seconds, according to scene motion. When the shutter button is pressed, we quickly analyze the scene motion statistics using the last 5 frames seen through the camera viewfinder and automatically determine a subsequent capture duration that aims to satisfy our frame selection criteria. We use a lightweight variant of the motion tracking and image alignment described in the next few sections, that operates in under 50ms, to compute an estimate of scene velocity. With this estimate, under a constant velocity assumption, we trivially derive a capture duration that yields the desired blur trail length (see Section 4.5). Given the extended capture duration of up to 7 seconds, we also derive a frame processing rate, to select an evenly distributed subset of up to 12 captured frames for processing, balancing the compute budget with a suitable temporal sampling rate. The captured frames selected for processing are queued up for immediate concurrent processing by the following stages, thereby hiding some of the processing latency during capture. 

##### 4.2 Automatic Subject Detection 

In the background blur case, we want the effect of a fixed subject with the rest of the world blurred behind them. Therefore, we automatically detect and track the main subject, and align the input frames to negate its motion. The subject is represented as a weight map, and is used in solving for the inverse subject motion alignment. 

The main subject is first predicted using the proxy task of attention saliency. For this task, we use a mobile-friendly 3-level U-Net with skip connections, with an encoder comprised of 15 BlazeBlock with 10 base channels [Bazarevsky et al. 2019] and a corresponding decoder made of separable convolutions and bi-linear upsampling layers. It is distilled from a larger model trained on the SALICON dataset [Jiang et al. 2015]. To focus on the peak of saliency in our signal, we re-normalize predicted values to the interval [0, 1] and zero out values below a threshold (we empirically chose 0.43). 

The saliency signal tends to peak on the subject center, so we complement it with a face signal, which helps keep subject faces sharp, which is especially important in subjects with complex articulated motion. We compute the face signal by first predicting human, cat, and dog face regions, then feathering the resulting regions using a smootherstep falloff [Ebert et al. 2003], and lastly masking it by a whole-subject segmentation similar to that of [Wadhwa et al. 2018]. 

We combine the saliency and face signals as follows, to produce the subject weight map with per pixel weight ğ‘¤ = ğ‘  (1 + ğ‘“ ), where ğ‘  âˆˆ [0, 1] is the saliency signal value and ğ‘“ âˆˆ [0, 1] is the face signal value, followed by a re-normalization to the interval [0, 1]. The face signal is also used in the compositing step to preserve face sharpness, as described in Section 4.8. 

##### 4.3 Motion Tracking 

We use a feature tracking library based on [Grundmann et al. 2011] for extracting the motion tracks used in subsequent image alignment. Motion track statistics are also used to select frames, to determine when sufficient motion has been captured in the scene.

Subject tracking in background blur requires a high concentration of tracks on the subject for stable, high quality alignments. As a latency optimization, we use rejection sampling over an image grid with cells of 5 x 5 pixels each, to generate feature tracks with density proportional to the subject weight map (Section 4.2). We only attempt to extract feature tracks in cells where a sampled uniform random variable ğ‘£ âˆˆ [0, 1] is smaller than the corresponding average track-weight at that grid location.

##### 4.4 Image Alignment 

Given the feature track correspondences from Section 4.3, we first estimate global transforms to align all the frames to our reference frame. This cancels out overall camera motion, including both handshake and sweeping motions used to track subjects. The remaining image alignment stages are specific to the desired motion-blur effect: foreground or background blur. For the purpose of illustration, we pick an example scene that could be rendered as either: a taxicab passing through a busy city intersection as shown in Figure 3. 

**4.4.1 Foreground blur.** To keep the background as sharp as possible, we must account for spatial effects such as parallax, rolling shutter, and lens distortion. After applying global transforms, we compute a residual vector as the position difference between a transformed tracked feature and its corresponding position in the base frame. We then use the residuals to estimate local refinement transforms on a grid of vertices across the image. The resulting spatially varying warp cancels motion in the background while retaining motion in the foreground, producing sharp backgrounds as in Figure 3 (bottom-left).

In [Zaragoza et al. 2013], the authors weight points by distance from each grid vertex to produce a spatially varying as-projective-aspossible warp. Our approach to placing a grid and estimating local transforms is similar, but we weight our points uniformly and use a hard cut-off for point inclusion during local similarity transform estimation for better performance. The estimation is controlled by the support radius of each vertex (shown as magenta circle in Figure 4), i.e. the maximum distance from a vertex that a feature point needs to be for inclusion in the local refinement estimation. We found that setting this radius to 1.5 times the cell size of the mesh grid and using a grid size of 8 x 6 cells, was large enough for the local refinement transforms to vary smoothly across the entire field-of-view, yet small enough that disparate scene objects from different parts of the scene do not affect each other. The estimated transforms are applied to each vertex to then generate a spatially varying mesh that aligns the background of any frame to that of the reference frame. To optimize latency, the application of this mesh is folded into the downstream rendering stage by passing a texture of local 2-d displacement vectors to the GPU. 

**4.4.2 Background blur**. In this case, as shown in Figure 3 (bottomright), we want the foreground to be as sharp as possible. We use the subject mask from Section 4.2 to select the subset of feature tracks that correspond to the foreground subject. With this as a starting point, we further use spectral clustering to select the most salient motion cluster to help discern the right motion segment to track and to remove outliers [Porikli 2004]. This is especially useful for articulated subjects, such as a running person whose limbs move differently from their torso. 

The goal of our objective function is to balance two goals: (1) subject sharpness: minimize the overall reprojection error of the salient point correspondences, and (2) temporal smoothness: keep the transformed background motion vectors as parallel as possible to those from the previous time step, as shown in Figure 5 and Figure 16. 

Given a pair of frames ğ‘– and ğ‘— with point correspondences xğ‘– and xğ‘— , we define a similarity transform that scales uniformly (ğ‘ ğ‘—,ğ‘– âˆˆ R 1 ), rotates (Rj,i âˆˆ ğ‘†ğ‘‚(2)) and translates (tj,i âˆˆ R 2 ) 2-dimensional points from frame ğ‘– to frame ğ‘— as follows. xË†j = ğ‘ ğ‘—,ğ‘–Rj,ixi + tj,i

For simplicity, we omit the from-and-to indices from the transform parameters ğ‘ , R and t and define our objective function as follows. minimize ğ‘ ,R,t ğœ†ğ‘“ ğ¸ğ‘“ (ğ‘ , R, t) + ğœ†ğ‘ğ¸ğ‘ (ğ‘ , R, t) (2)

The scalars ğœ†ğ‘“ and ğœ†ğ‘ are the relative weights of the objective function terms. The subject sharpness term ğ¸ğ‘“ is defined on the foreground points x âˆˆ Xğ‘“ as the L-2 norm of the reprojection error of transformed point correspondences (using Eq. 1) as follows. ğ¸ğ‘“ (ğ‘ , R, t) = âˆ‘ï¸ xâˆˆXğ‘“ âˆ¥xj âˆ’ ğ‘ Rxi âˆ’ tâˆ¥2 (3) 

The background term ğ¸ğ‘ is used as a temporal smoothness prior to penalize background flow vectors that are not parallel to their counterparts from the previous frame pair. Given three distinct frame indices ğ‘–, ğ‘— and ğ‘˜, this is defined using vector dot product as a measure of parallelism as follows: ğ¸ğ‘ (ğ‘ ğ‘–,ğ‘˜, Rğ‘–,ğ‘˜, tğ‘–,ğ‘˜ ) = âˆ‘ï¸ xâˆˆXğ‘ smooth ğ¿1 |vğ‘–,ğ‘— Â· vğ‘—,ğ‘˜ | âˆ¥vğ‘–,ğ‘— âˆ¥ âˆ¥vğ‘—,ğ‘˜ âˆ¥ (4)

where vğ‘,ğ‘ = xË†ğ‘ âˆ’xğ‘ is the directional displacement vector between a point in the reference frame ğ‘ and its transformed point correspondence from frame ğ‘. The smooth-ğ¿1 loss function from [Huber 1964] is used to prevent vanishing gradients during the optimization. smooth ğ¿1 (ğ‘¥) = ( 0.5ğ‘¥ 2 , if |ğ‘¥ | < 1 |ğ‘¥ | âˆ’ 0.5, otherwise (5) 

Without loss of generality, we chose ğ‘– = 0 as the fixed reference frame index, and set ğ‘˜ = ğ‘— + 1 in Eq. 4 to estimate the transforms incrementally for each new frame pair (ğ‘—, ğ‘— + 1) in a single forward pass through the frame sequence. We solved this non-linear optimization problem using the Ceres library [Agarwal et al. 2022]. We use values 1 and 10 for ğœ†ğ‘“ and ğœ†ğ‘ respectively, to balance the effect of both error terms. 

The temporal regularization term ğ¸ğ‘ is not defined for the first frame pair, which we handle as a special case using a constraint on the scale and rotation parameters, ğ‘  and R, from Eq. 2. Our key observation comes from subjects of background blur shots that undergo rotational motion in the image plane. Simply inverting this rotation produces undesirable multiple sharp regions in the result, as shown in Figure 5. In traditional panning photography, it is uncommon to attempt the rotation blur effect and exceedingly difficult to achieve subject sharpness in this manner (i.e. rotating the camera around an axis centered away from the subject). Instead, it is typically done panning the camera, tracking the overall trajectory of the subject, and our method aims for these outcomes. 

To estimate the initial scale and rotation, we use the integrated estimation technique from [ZinÃŸer et al. 2005]. We then constrain the estimated rotation, R in Eq. 2, to prevent any additional sharp regions from detracting away from the sharp subject. We empirically found that constraining the roll angle to 25% of its estimated value helps make the blur field more linear, as shown in Figure 5, with only the subject kept sharp as desired. More examples of image alignment for background-blur scenes are provided in Section 5.2. 

##### 4.5 Frame Selection 

Our system uses a frame selection mechanism that computes an estimate of motion-blur trailsâ€™ length, to decide when the incremental frame processing outer-loop should stop (see Section 3). First, we use the transformations computed by the alignment solver to transform the motion feature tracks to the reference space of the base frame, where they align spatially with the corresponding tracked featuresâ€™ motion-blur trails in the output image. The length of each aligned track can then be computed, and we use a high percentile of the track length distribution as an estimate of overall blur trail length. This estimate is finally compared to a constant target setting, to decide whether the frame selection criteria is satisfied. 

We measure the track length in percentage of image diagonal, a metric that is largely insensitive to image resolution or aspect-ratio. In the case of foreground blur, our criteria is for the 98th percentile to reach a target of 30%, producing relatively long and smooth blur trails for the fastest moving object. In the background blur case, we use the 80th percentile and a target of 2.8%, producing short blur trails for a larger area of the background, aiming to preserve subject sharpness and avoid losing the context of the surrounding scene. These settings were derived empirically, iterating over large collections of input bursts. 

##### 4.6 Motion Prediction 

Once the input low-resolution images are aligned, we feed them through a motion-blur kernel-prediction neural network, one input frame pair at a time, predicting a pair of line and weight kernel maps at each iteration. The low-resolution kernel maps are used to synthesize motion-blur segments at half resolution, spanning the corresponding input frames, as described in Section 4.7. 

The motion prediction model is responsible for predicting the parameters of two spatial integrals along line segments, which approximate the temporal integral defining the averaging of colors seen through each motion-blurred output pixel, during the corresponding time interval. We use a model based on [Brooks and Barron 2019], with further modifications that improve the trade-off between performance and image quality, allowing us to fit within a reasonable memory and compute budget on mobile devices.

Their mathematical formulation predicts weight maps ğ‘Šğ‘– per input frame ğ‘– in a given image pair ğ‘˜, with ğ‘ = 17 channels, which are used to weigh each corresponding texture sample along the predicted line segments. We simplify this model by predicting only a single channel, used to weigh the result of the integral from each input frame. An example gray-scale map can be seen in Figure 2, showing that the network predicts approximately equal weights everywhere across input images, except in areas of dis-occlusion where the weights favor the result from one of the two inputs. This simplification significantly reduces system complexity and memory use, and allows for more of the network capacity to be devoted to predicting the line segments. 

In addition, we eliminate artifacts due to the predicted line segmentsâ€™ endpoint error [Zhang et al. 2016], causing them to meet imperfectly at the end of the spanned time interval, and resulting in very noticeable artifacts in the middle of blur trails, as illustrated in Figure 6. To avoid this issue, we scale the input image texture samples further by a normalized decreasing linear ramp functionğ‘¤ğ‘›, that favors samples close to the output pixel and gradually downweighs samples further away along each predicted line segment. The intensity of the output pixel (ğ‘¥, ğ‘¦) for the input frame pair ğ‘˜ is: ğ¼ğ‘˜ (ğ‘¥, ğ‘¦) = âˆ‘ï¸ ğ‘–âˆˆ {ğ‘˜,ğ‘˜+1} ğ‘Šğ‘–(ğ‘¥, ğ‘¦) Ãğ‘ âˆ’1 ğ‘›=0 ğ‘¤ğ‘› ğ‘ âˆ‘ï¸âˆ’1 ğ‘›=0 ğ‘¤ğ‘› ğ¼ğ‘–(ğ‘¥ğ‘–ğ‘›, ğ‘¦ğ‘–ğ‘›) (6) with ğ‘¤ğ‘› = 1 âˆ’ ğ‘›/ğ‘, and with sampled positions: ğ‘¥ğ‘–ğ‘› = ğ‘¥ + ( ğ‘› ğ‘ âˆ’ 1 ) Î” ğ‘¥ ğ‘– (ğ‘¥, ğ‘¦) and ğ‘¦ğ‘–ğ‘› = ğ‘¦ + ( ğ‘› ğ‘ âˆ’ 1 ) Î” ğ‘¦ ğ‘– (ğ‘¥, ğ‘¦) where Î”ğ‘– are the predicted line segments. 

We also modify the network architecture as follows. First, we replace the leaky ReLU convolution activations throughout, with a parameterized ReLU [He et al. 2015], where the slope coefficient is learned. Next, to avoid common checkerboard artifacts [Odena et al. 2016], we replace the 2x resampling layers to use average pooling for downsampling, and bi-linear upsampling followed by a 2x2 convolution. This results in a model labeled "Ours-large" analyzed in Section 5. Furthermore, to improve the balance between the number of floating operations, number of parameters and receptive field, we further reduce the U-Net model topology to only 3 levels, where each level is using a 1x1 convolution, followed by a ResNet block [He et al. 2016] with four 3x3 convolution layers. This results in a model labeled "Ours" with significantly fewer learned parameters. 

As shown in Figure 6, the ramp function ğ‘¤ğ‘› brings a significant benefit to our learned single weight model, as it causes the predicted line segments to span spatially in each input image, the equivalent of the full time interval being integrated. When our model is trained with this term ablated, resulting in the model "Ours-abl.", the network predicts line segments that span approximately half of the time interval on each side, causing the noticeable discontinuity in the middle of blur trails. More examples can be found in the model comparison analysis provided in Section 5. 

##### 4.7 Rendering 

The line and weight kernel maps output by the motion prediction network are used by a renderer that synthesizes the motion-blurred image. The renderer is implemented in an OpenCL kernel, which runs very efficiently on the mobile deviceâ€™s GPU, leveraging the hardware texturing units while adaptively sampling texture lookups in the half resolution input images (the number of texture samples ğ‘ is adjusted proportionally to the length of the predicted line vectors). Motion prediction and rendering iterations can be performed one input frame-pair at a time, producing piecewise-linear motion-blur trails. Kernel maps are up-sampled from low to half-resolution by using bi-linear texture lookups. 

**4.7.1 Spline interpolation**. Piecewise-linear motion interpolation may introduce jagged visual artifacts in motion trails. To interpolate the motion more smoothly, we interpolate the inferred instantaneous flow Î”ğ‘– between frames using cubic Hermite splines. 

The instantaneous flow ğ›¿ğ‘– at each pixel is inferred by constructing a vector ğ»(Î” + ğ‘– , Î” âˆ’ ğ‘– ) parallel to (Î” + ğ‘– + Î” âˆ’ ğ‘– ), with magnitude equal to the harmonic mean of |Î” + ğ‘– | and |Î” âˆ’ ğ‘– |. Superscripts + and âˆ’ refer to time directions. If Î” + ğ‘– and Î” âˆ’ ğ‘– deviate by an angle ğœƒ from a straightline path, the vector is further scaled by a factor of (ğœƒ/sinğœƒ) for smaller angular deviations (< 90Â°), tapering this adjustment back towards zero for larger deviations (where the path doubles back acutely) to avoid singularities. These correctional factors reduce overshoot, and keep the parametric spline speed more stable for regions of moderate curvature. ğ›¿ğ‘– = ğ»(Î” + ğ‘– , Î” âˆ’ ğ‘– ) (ğœƒ/sinğœƒ) Ã— ( 1, ğœƒ â‰¤ ğœ‹/2 1 âˆ’ (2ğœƒ/ğœ‹ âˆ’ 1) 4 , ğœƒ > ğœ‹/2 (7) 

For the accumulated blur of ğ¼ğ‘˜ on the interval [k .. k+1] for output position (x, y), we solve for a parametric 2D cubic spline path ğœŒ(x, y, t) satisfying four constraints: â€¢ ğœŒ(x, y, 0) = (x, y) â€¢ ğœŒ(x, y, 1) = (x, y) + Î” + ğ‘– (ğ‘¥, ğ‘¦) â€¢ ğœŒ â€² (ğ‘¥, ğ‘¦, 0) = ğ›¿ğ‘–(ğœŒ (ğ‘¥, ğ‘¦, 0)) â€¢ ğœŒ â€² (ğ‘¥, ğ‘¦, 1) = ğ›¿ğ‘–+1 (ğœŒ (ğ‘¥, ğ‘¦, 1)) 

We then accumulate the blur along this path by sampling uniformly in parameter space, normalizing the weight of each sample to compensate for the non-uniform spatial sampling in image space in order to ensure spatially uniform brightness along motion trails. At the burst endpoints we extrapolate the flow beyond the first and last frames by attempting to preserve the curvature of the flow through those endpoints. As shown in Figure 7: if â€™Câ€™ represents the final frame in a burst, a motion trail position at the "next" frame D is extrapolated by reflecting A in the line bisecting BC (constructing Aâ€™), then clamping the magnitude of CAâ€™ to |BC| to form CD. The flow at C is then inferred from points {B,C,D}. 

**4.7.2 Frame accumulation**. In practice, the blur is accumulated in several passes: two passes per frame pair, weighted to fall off linearly between one frame and the next. For an output pixel at position p at frame ğ¼ğ‘– , the blur between frame ğ¼ğ‘– and ğ¼ğ‘–+1 is accumulated by using the aforementioned flow splines to determine the projected position pâ€™ in frame ğ¼ğ‘– at relative time t. For K frame pairs in the burst, 2K such passes (K forward, K backward) are computed and summed to produce the final blur result. For each temporal direction: ğ¼(ğ‘¥, ğ‘¦) = ğ¾ âˆ‘ï¸âˆ’1 ğ‘–=0 ğ‘ âˆ‘ï¸âˆ’1 ğ‘›=0 ğ¼ğ‘–(ğœŒğ‘–(ğ‘¥, ğ‘¦, ğ‘¡ğ‘›)) |ğœŒ â€² ğ‘– (ğ‘¥, ğ‘¦, ğ‘¡ğ‘›)| ğ‘¤ğ‘› (8) 

4.7.3 Soft Gamma Colorspace. Very bright highlights (e.g. car headlights) tend to saturate the camera sensor, resulting in their blurred motion trails becoming unrealistically dim even when processed in linear colorspace. The clipping is due to the finite range of the input sensor, and the brightness loss becomes noticeable when the clipped input highlight energy is distributed (i.e. synthetically motion-blurred) over many output pixels. 

To work around this limitation, we process the blur in an intentionally non-linear colorspace, using an invertible gamma-like "soft gamma" function ğ›¾ğ‘  , shown in Figure 8, on the interval [0..1]. 

This adjusts the brightness curve in the opposite direction from a linear-to-sRGB color transformation, emphasizing highlights without crushing shadows, allowing the nonlinear frames to be stored with usable fidelity in 16-bit buffers. The function is applied to the warped downsampled 2x buffers on creation, using a value of 3.0 for ğ‘˜, and is later inverted (by reapplying with ğ‘˜ = 1.0/3.0) after accumulating the blur for all frames. (See ablation in Section 5). ğ›¾ğ‘  (ğ‘£) = ğ‘£ ğ‘£ + (1 âˆ’ ğ‘£) ğ‘˜ â‰ˆ ğ‘£ ğ‘˜ (9) 

This is homologous to the Bias family of functions in [Schlick 1994], but our reparameterization in Eq. 9 makes clearer the connection to the corresponding gamma curve with exponent ğ‘˜. The idea of processing the blur in the modified colorspace was inspired by the Ordinal Transform technique in [Weiss 2006]. Our goal is similar to the clipped highlight recovery technique in [Lancelle et al. 2019], which in comparison uses a more abrupt discontinuous highlight boosting function, that may further clip the signal.

##### 4.8 Compositing 

The synthetically blurred image described in Section 4.7 is computed at half resolution to satisfy device memory and latency constraints. Accordingly, even perfectly aligned, zero-motion regions of the blurred image will lose detail due to the upsampling of the result computed at half resolution. To preserve details, we composite the blurred image with a maximally sharp regular exposure where we expect things to be sharp. Two categories need this protection: 1) stationary scene content, and 2) semantically important subjects with little movement, as shown in Figure 9. 

For category 1, we produce a mask of pixels with very little motion across the entire set of frame pairs, ğ‘€flow:

(1) Compute a per-pixel maximum motion magnitude |ğ¹ | across all frame pairs.
(2) Compute a reference motion magnitude |ğ¹ |ref thatâ€™s effectively a robust max over all pixels in |ğ¹ | (i.e., 99th percentile). 
(3) Rescale and clamp the per-pixel motion magnitudes such that anything below ğ›¼|ğ¹ |ref is mapped to 0 and anything above ğ›½|ğ¹ |ref is mapped to 1. We use values 0.16 and 0.32 for ğ›¼ and ğ›½ respectively. ğ‘€flow = |ğ¹ | âˆ’ ğ›¼|ğ¹ |ref ğ›½|ğ¹ |ref âˆ’ ğ›¼|ğ¹ |ref 
(4) Apply a bilateral blur using the sharp image as a guide [He et al. 2013], to ensure that any edges in ğ‘€flow correspond to real edges and minimize artifacts where the flow field is unreliable (e.g., uniform or texture-less regions like skies). 

Category 2 is more complicated and breaks from the physical behavior of optical motion blur in favor of aesthetics. E.g., if a scene has two subjects moving with different trajectories, it would be impossible to sharply align on both simultaneously. Even a single subject can be impossible to align due to movement within the subject, e.g., changes in facial expression, etc. An image with a blurry subject face is a (bad) blurry image. Our solution is to reuse the semantic face signal described in 4.2, modified to only include the faces that have low average feature movement in the aligned reference frame. 

Finally, we combine the flow and clipped face masks with a simple max operator. Figure 9 shows the cumulative effect of the two mask types on the final composite.

#### 5 RESULTS 

Figure 10 shows several foreground and background blur typical use cases, captured and processed using our system. The bursts were all captured hand-held and the results were generated fully automatically, without the need to adjust any settings. In both cases, what makes these long exposure photographs successful is the presence and contrast between sharp and blurry elements. 

The on-device latency of our system varies according to the number of frames selected for processing. The latency for the main stages (see Figure 2), measured on a Google Pixel 7 device, are as follows. Subject detection, including 8x downsampling and tone-mapping of the base frame: 330ms; motion tracking and alignment, including 8x downsampling and tone-mapping: 55ms per frame; inter-frame motion prediction, including concurrent 2x downsampling and RAW to linear conversion: 77ms per selected frame pair; rendering: 42ms per selected frame pair; final upsampling, compositing and tonemapping of both image results: 790ms. In the background blur case, a small number of frames are typically selected (e.g. 4), leading to a short total latency (e.g. 1.7s). In the foreground blur case, a higher number of frames are typically selected (e.g. 12) but most of the processing is happening during the extended capture (see Section 4.1) and the latency is therefore largely hidden from the user. 

##### 5.1 Track Weights Comparison 

In the following ablation, we compare the effect of including faceregion upweighting in motion track weight maps for backgroundblur alignment quality. (Please refer to Section 4.2 for more details). 

We find that including both gaze saliency and face detections in the motion track weight map benefits image subjects with complex articulated motion (which can cause the wrong part of the subject to be tracked). A representative example is shown in Figure 11.

##### 5.2 Image Alignment Comparison 

In Figure 16, we showcase additional examples of image alignment on background-blur scenes, comparing the aesthetics of results when the regularization term ğ¸ğ‘ from Eq. 2 is excluded (left) and included (right). In the left-hand side column of Figure 16, we observe that optimizing just for the subjectâ€™s sharpness ğ¸ğ‘ doesnâ€™t account for the background of the scene. Consequently, sudden changes in transform parameters over time are allowed, resulting in different parts of the field of view having motion blur in completely different directions. By adding the temporal regularization term ğ¸ğ‘ , we get the results on the right-hand side column of Figure 16 with consistent blur trails. The second example showcases the effect of dampening the rotational parameter, avoiding the blur vortex (green insets).

##### 5.3 Motion Prediction Comparison 

We compare models described in Section 4.6 with those from [Brooks and Barron 2019] that use uniform weights, labelled "BB19-uni.", and that learn ğ‘ = 17 weights per input image, labelled "BB19". 

All the compared models were trained with the same hyperparameters described in [Brooks and Barron 2019]. To supervise the training, we generate a bracketed dataset of input image triplets from many videos, as described in [Reda et al. 2022], synthesizing the pseudo ground-truth motion-blurred image using a previously trained FILM frame interpolation model. To evaluate our model we used a test set with 2000 examples and report the PSNR and SSIM, i.e. comparing the synthesized motion-blur image to the pseudo ground-truth, in Table 1. 

A visual comparison on 512 x 384 image crops is provided in Figure 17 and shows that our model performs visually similarly to "BB19" (e.g. blur smoothness, handling of dis-occlusions), despite the significant simplifications of our implementation to run on mobile devices. It also reveals that both models "Ours-abl." and "BB19-uni." suffer from the same discontinuity artifacts in the middle of blur trails, which are described in Section 4.6. Our model runs in under 80ms on a Google Pixel 7 mobile deviceâ€™s TPU [Gupta 2021], when given an input image pair from our low resolution image pipeline.

##### 5.4 Rendering Comparison 

**5.4.1 Motion Interpolation**. In Figure 12, we compare the effects of piecewise-linear flow interpolation vs cubic spline interpolation. Particularly when camera or object motion from frame to frame is irregular, spline interpolation can impart a much more natural and photorealistic appearance to the motion trails.

**5.4.2 Rendering Colorspace.** In Figure 13, we compare the results of performing the blurring operation in a conventional sRGB colorspace, versus a linear physically correct colorspace, versus our non-physical "soft gamma" colorspace, obtained by adjusting the linear-space image in a direction opposite from a usual linear to sRGB color transformation. The figure illustrates how blurring in the soft-gamma colorspace emphasizes and preserves the brightness of the motion trails in the highlights and generally increases their contrast and color saturation. 

##### 5.5 Comparison to Mobile Phone Camera Applications 

Unlike other works which realize a long exposure effect [Lancelle et al. 2019; Lee et al. 2009; Luo et al. 2018, 2020; Mikamo et al. 2021; Teramoto et al. 2010], our pipeline is a responsive mobile phone capture experience. Therefore, we also compare our results to released capture experiences for consumer phones. 

Several mobile applications allow a more manual control of the capture schedule on mobile phones such as Even Longer, Moment, Neoshot, and Procam 8 (all available in the iOS App Store). These apps do not seem to have frame alignment capability, and therefore require a tripod for capturing sharp long exposure images. Spectre, released on iOS, seems to have stabilization and auto-exposure capabilities. Through a capture study of dozens of scenes, we found the hand-held performance of Spectre to be inconsistent. Figure 14 shows representative comparisons of the results of our pipeline with Spectre. 

To our knowledge, our pipeline is the only mobile-phone capture experience with all of the following features: background-blur alignment (automatically tracking and keeping a moving subject sharp), robust foreground-blur alignment (keeping the background sharp), motion interpolation for smooth motion trails (compared to results showing temporal undersampling), and face-region sharpness protection (keeping slightly moving subjects sharp).

##### 5.6 Evaluation on Existing Datasets 

In Figure 15, we evaluate our method on the publicly available video dataset in [Liu et al. 2014]. The images are also available in the supplement, and can be compared to their results (see their Figure 9), as well as results in [Lancelle et al. 2019] (see their Figure 15). 

Our automatic subject detection aligns the result on peopleâ€™s faces when they are detected, and on visually salient features otherwise, which matches the selected subject from previous work in many scenes. When multiple faces are detected, our method chooses to align on the largest face, which may lead to a different outcome (Figure 15a middle-right and lower-right examples). We also observe a possible mismatch when no faces are present or are too small to be detected, e.g. while our saliency signal reacts to the most colorful or brightest areas of the image (Figure 15a lower-left and lower-middle examples respectively). 

Even though we use a simple 2D image alignment approach (see Section 4.4.2), our method leads to comparable subject stabilization in most cases. Our similarity transform solver is able to preserve subject sharpness and models a relative virtual camera motion that is similar to that of compared works and is sometimes more accurate (Figure 15a center and Figure 15b right examples).

Our rendering approach is most similar to [Lancelle et al. 2019] but differs in the implementation to interpolate motion between frames. Our method scales to very high resolution images in an efficient manner on a mobile device, and we find the resulting motion-blur quality to be comparable. Both works benefit from integrating motion-blur from the input images spanning the whole time-interval, unlike the approach in [Liu et al. 2014], which uses a spatially-varying blur of only the base frame. Our method correctly renders the dynamic motion of the scene handling dis-occlusions, showing other moving scene objects, peopleâ€™s moving limbs, and the motion of the background, all relative to the subject and as seen through the virtual camera aligned over time. In contrast, blurring only the base frame assumes the scene is static and only the aligned camera transformation affects the blur. This is most noticeable when comparing our turtle result in Figure 15b-left to theirs. Our system renders the relative coral motion direction correctly, as can be seen in the input video, and shows the turtleâ€™s moving fin and the swirling individual motion of surrounding fish. 

The amount of blur is normalized by our frame selection algorithm described in Section 4.5. Our stylistic background blur length target is shorter than the results in [Lancelle et al. 2019], and is motivated by the goal to preserve subject sharpness and scene context. 

#### 6 LIMITATIONS AND FUTURE WORK 

Background blur scenes with very small subjects tend to significantly increase the occurrence of saliency and portrait mask mispredictions and feature tracking errors, ultimately resulting in an undesirable alignment and preserving the sharpness of the incorrect image region. Although our system can handle reasonably small subjects, this problem can be improved further by refining these predictions using the appropriate sub-region of the input images down-sampled at a higher resolution. 

Our motion prediction model with receptive field window of 128 pixels can handle approximately 64 pixels of motion disparity at the chosen input low resolution. In our system, this corresponds to 512 pixels of disparity at full resolution, which is a good practical upper bound when capturing 12 megapixel bursts at 30fps. Larger motion disparities across frame pairs cause inaccurate predicted kernel maps and result in significant artifacts in the synthesized motionblur. When these rare cases are detected in the motion analysis stage of our pipeline, we decide to output only the sharp exposure. 

None of the models we tested perfectly handle motion silhouettes, when the foreground and background are moving between perpendicular and opposite directions or in the presence of large motion, causing swirly looking or disocclusion artifacts. We also notice similar artifacts in areas where a cast shadow moves in a different direction than the object where the shadow is projected onto. Some examples can be found in the supplementary material in Figure 1. Resolving these challenging cases is left for future work. 

#### 7 CONCLUSION 

In this paper, we described a long exposure computational photography system, that is able to produce high quality long exposure foreground or background blur effects. Our system operates in a smartphone camera app and outputs both long and conventional exposures fully automatically, in just a few seconds after pressing the shutter button. We described the key elements that make our system successful: adapting the burst capture schedule to scene motion velocity, separating the main subject from the background and tracking their motion, creating custom aesthetic alignments of input burst images, synthesizing smooth curved motion-blur spanning multiple underexposed HDR sharp input images, and compositing sharp and motion-blurred results to protect important scene locations - exceeding what would be physically possible with traditional long-exposure photography. The careful combination of these elements gives our system the ability to produce aesthetically pleasing blur trails and to preserve sharpness where it is most needed. The end-to-end integration of our system into a consumer mobile device camera makes it possible for casual users to access a creative style previously reserved to more advanced photographers.