{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from get_embedding import getEmbeddingDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "book_data = pd.read_excel(\"Book.xlsx\")\n",
    "book_df = book_data[['tags_emotion', 'label']]\n",
    "book_test_df = pd.read_excel(\"Book_test.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/cq1/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(((91, 1), (91, 10)), ((23, 1), (23, 10)), (192, 1))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize the OneHotEncoder and Tokenizer\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "\n",
    "# One-hot encode the labels\n",
    "labels = onehot_encoder.fit_transform(book_df['label'].values.reshape(-1, 1))\n",
    "\n",
    "# Fit the tokenizer on the text data\n",
    "tokenizer.fit_on_texts(book_df['tags_emotion'])\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(book_df['tags_emotion'])\n",
    "sequences_test = tokenizer.texts_to_sequences(book_test_df['tags_emotion'])\n",
    "\n",
    "# Pad the sequences so that they are all the same length\n",
    "max_sequence_length = max(max(len(seq) for seq in sequences), max(len(seq) for seq in sequences_test))\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the \"Book\" data into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (padded_sequences_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((torch.Size([91, 1]), torch.Size([91, 10])),\n",
       " (torch.Size([23, 1]), torch.Size([23, 10])),\n",
       " torch.Size([192, 1]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "encoded_labels = label_encoder.fit_transform(book_df['label'])\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "labels_one_hot = torch.nn.functional.one_hot(torch.tensor(encoded_labels))\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(book_df['tags_emotion'])\n",
    "sequences = tokenizer.texts_to_sequences(book_df['tags_emotion'])\n",
    "sequences_test = tokenizer.texts_to_sequences(book_test_df['tags_emotion'])\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = max(max(len(seq) for seq in sequences), max(len(seq) for seq in sequences_test))\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_sequence_length)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "X_test = torch.tensor(padded_sequences_test, dtype=torch.long)\n",
    "y = labels_one_hot.float()\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDatasets for training and validation\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "\n",
    "# DataLoader parameters\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43478260869565216"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer and LabelEncoder\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the labels to integers\n",
    "encoded_labels = label_encoder.fit_transform(book_df['label'])\n",
    "\n",
    "# Create a TF-IDF representation of the text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(book_df['tags_emotion'])\n",
    "\n",
    "# Split the data into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(tfidf_matrix, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_score = rf_classifier.score(X_val, y_val)\n",
    "\n",
    "val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags_emotion</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>压迫</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>压抑</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>残忍</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>残暴</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>兴奋</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tags_emotion  predicted_label\n",
       "0           压迫                5\n",
       "1           压抑                5\n",
       "2           残忍                5\n",
       "3           残暴                5\n",
       "4           兴奋                2"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the \"Book_test\" data using the fitted TF-IDF vectorizer\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform(book_test_df['tags_emotion'])\n",
    "\n",
    "# Predict the labels for the \"Book_test\" data\n",
    "test_predictions = rf_classifier.predict(tfidf_matrix_test)\n",
    "\n",
    "# Convert the integer predictions back to the original labels\n",
    "test_predicted_labels = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# Add the predicted labels to the test dataframe\n",
    "book_test_df['predicted_label'] = test_predicted_labels\n",
    "\n",
    "# Show the first few rows of the dataframe with predictions\n",
    "book_test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_test_df.sort_values('predicted_label').to_excel('Book_draft.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3/3 [==============================] - 1s 55ms/step - loss: 2.2940 - accuracy: 0.2088 - val_loss: 2.2853 - val_accuracy: 0.2609\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.2663 - accuracy: 0.3736 - val_loss: 2.2759 - val_accuracy: 0.2609\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.2414 - accuracy: 0.4396 - val_loss: 2.2659 - val_accuracy: 0.3043\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.2137 - accuracy: 0.5275 - val_loss: 2.2557 - val_accuracy: 0.3043\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.1865 - accuracy: 0.5385 - val_loss: 2.2450 - val_accuracy: 0.3478\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.1552 - accuracy: 0.5385 - val_loss: 2.2340 - val_accuracy: 0.3478\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.1226 - accuracy: 0.5604 - val_loss: 2.2223 - val_accuracy: 0.3478\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.0862 - accuracy: 0.5604 - val_loss: 2.2105 - val_accuracy: 0.3913\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.0471 - accuracy: 0.5604 - val_loss: 2.1982 - val_accuracy: 0.3913\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.0033 - accuracy: 0.5714 - val_loss: 2.1853 - val_accuracy: 0.4348\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.9563 - accuracy: 0.5714 - val_loss: 2.1722 - val_accuracy: 0.4348\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.9036 - accuracy: 0.6044 - val_loss: 2.1585 - val_accuracy: 0.4348\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.8484 - accuracy: 0.6593 - val_loss: 2.1445 - val_accuracy: 0.4348\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.7867 - accuracy: 0.6923 - val_loss: 2.1304 - val_accuracy: 0.4348\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.7216 - accuracy: 0.7253 - val_loss: 2.1161 - val_accuracy: 0.4348\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.6519 - accuracy: 0.7692 - val_loss: 2.1019 - val_accuracy: 0.4348\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.5779 - accuracy: 0.8242 - val_loss: 2.0879 - val_accuracy: 0.4348\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.5006 - accuracy: 0.8571 - val_loss: 2.0743 - val_accuracy: 0.4348\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.4235 - accuracy: 0.8901 - val_loss: 2.0614 - val_accuracy: 0.4348\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.3440 - accuracy: 0.9011 - val_loss: 2.0488 - val_accuracy: 0.4348\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.2663 - accuracy: 0.9121 - val_loss: 2.0371 - val_accuracy: 0.4348\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.1858 - accuracy: 0.9341 - val_loss: 2.0267 - val_accuracy: 0.4348\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1092 - accuracy: 0.9341 - val_loss: 2.0172 - val_accuracy: 0.4348\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.0350 - accuracy: 0.9451 - val_loss: 2.0090 - val_accuracy: 0.4348\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.9613 - accuracy: 0.9670 - val_loss: 2.0019 - val_accuracy: 0.4348\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8919 - accuracy: 0.9780 - val_loss: 1.9962 - val_accuracy: 0.4348\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.8259 - accuracy: 0.9890 - val_loss: 1.9916 - val_accuracy: 0.3913\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.7594 - accuracy: 0.9890 - val_loss: 1.9874 - val_accuracy: 0.3913\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6983 - accuracy: 0.9890 - val_loss: 1.9842 - val_accuracy: 0.3478\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6435 - accuracy: 0.9890 - val_loss: 1.9818 - val_accuracy: 0.3478\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data here\n",
    "book_df = pd.read_excel('Book.xlsx')\n",
    "book_test_df = pd.read_excel('Book_test.xlsx')\n",
    "\n",
    "# Assume book_df has columns 'tags_emotion' and 'label'\n",
    "texts = book_df['tags_emotion'].values\n",
    "labels = book_df['label'].values\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_length))\n",
    "model.add(SimpleRNN(units=64, return_sequences=False))\n",
    "model.add(Dense(units=y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val))\n",
    "model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 619us/step\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and pad the 'Book_test' text data\n",
    "test_texts = book_test_df['tags_emotion'].values\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_length)  # max_length used during training\n",
    "\n",
    "# Predict with the model\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# If your model outputs one-hot encoded predictions, you'll need to convert these to label encodings\n",
    "predicted_labels = predictions.argmax(axis=-1)\n",
    "\n",
    "# Convert the integer predictions back to the original labels using the label encoder\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# Add the predicted labels to the test dataframe if needed\n",
    "book_test_df['predicted_label'] = predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "diction = {1:'搞笑组', 2:'情绪组', 3:'害怕组', 4:'正能量组', 5:'负能量组', 6:'烂片组', 7:'题材', 8:'主角路线', 9:'励志组', 10:'荒诞组', 11:'思考组', 12:'经典组'}\n",
    "\n",
    "df = book_test_df.sort_values('predicted_label')\n",
    "df['predicted_label'] = df['predicted_label'].map(diction)\n",
    "df = df[['tags_emotion', 'predicted_label']]\n",
    "df.to_excel('Book_output.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags_emotion</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>幽默</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>幸福</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>欢乐</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>轻松</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>欢笑</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>温馨</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>感人</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>兴奋</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>喜悦</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>开心</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>治愈</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>快乐</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>惊慌</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>紧张</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>恐慌</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>感激</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>信任</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>乐观</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>友爱</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>合作</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>荣耀</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>奋斗</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>忠诚</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>勇敢</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>英勇</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>和谐</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>坚强</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>求生</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>阴森</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>善良</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>守护</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>好奇</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>训练</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>禁赛</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>贫困</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>浪漫</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>解谜</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>冷酷</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>压迫</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>欺压</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>重逢</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>嚣张</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>夺船</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>回忆</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>利用</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>讽刺</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>考验</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>争执</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>胜利</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>违法</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tags_emotion  predicted_label\n",
       "128           幽默                1\n",
       "29            幸福                2\n",
       "45            欢乐                2\n",
       "118           轻松                2\n",
       "44            欢笑                2\n",
       "80            温馨                2\n",
       "28            感人                2\n",
       "4             兴奋                2\n",
       "97            喜悦                2\n",
       "188           开心                2\n",
       "35            治愈                2\n",
       "46            快乐                2\n",
       "20            惊慌                3\n",
       "177           紧张                3\n",
       "21            恐慌                3\n",
       "166           感激                4\n",
       "72            信任                4\n",
       "169           乐观                4\n",
       "171           友爱                4\n",
       "34            合作                4\n",
       "131           荣耀                4\n",
       "185           奋斗                4\n",
       "147           忠诚                4\n",
       "10            勇敢                4\n",
       "9             英勇                4\n",
       "162           和谐                4\n",
       "54            坚强                4\n",
       "132           求生                5\n",
       "139           阴森                5\n",
       "138           善良                5\n",
       "127           守护                5\n",
       "129           好奇                5\n",
       "136           训练                5\n",
       "135           禁赛                5\n",
       "134           贫困                5\n",
       "133           浪漫                5\n",
       "137           解谜                5\n",
       "126           冷酷                5\n",
       "0             压迫                5\n",
       "124           欺压                5\n",
       "105           重逢                5\n",
       "106           嚣张                5\n",
       "107           夺船                5\n",
       "108           回忆                5\n",
       "109           利用                5\n",
       "110           讽刺                5\n",
       "111           考验                5\n",
       "112           争执                5\n",
       "125           胜利                5\n",
       "113           违法                5"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_test_df.sort_values('predicted_label').head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags_emotion</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tags_emotion, predicted_label]\n",
       "Index: []"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(book_test_df[book_test_df['predicted_label']==12]))\n",
    "book_test_df[book_test_df['predicted_label']==12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags_emotion</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tags_emotion, predicted_label]\n",
       "Index: []"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_test = pd.read_excel('Book_draft.xlsx')\n",
    "print(len(book_test[book_test['predicted_label']==12]))\n",
    "book_test[book_test['predicted_label']==12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抑郁\n",
      "友爱\n",
      "欢乐\n",
      "感动\n",
      "腐败\n",
      "无助\n",
      "自省\n",
      "复仇\n"
     ]
    }
   ],
   "source": [
    "for i in b:\n",
    "    if i not in a:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags_emotion</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>奋斗</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tags_emotion  predicted_label\n",
       "26           奋斗                4"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_test[book_test['tags_emotion'] == '奋斗']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_pca_embedding_dict = {k: tuple(v) for k, v in diction.items()}\n",
    "inverse_pca_embedding_dict = {v: k for k, v in tuple_pca_embedding_dict.items()}\n",
    "book_test_df['predicted_label'] = book_test_df['predicted_label'].apply(lambda x: inverse_pca_embedding_dict[tuple(x)] if tuple(x) in inverse_pca_embedding_dict else None)\n",
    "book_test_df = book_test_df[['tags_emotion', 'predicted_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags_emotion</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>幽默</td>\n",
       "      <td>搞笑组</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>温馨</td>\n",
       "      <td>情绪组</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>开心</td>\n",
       "      <td>情绪组</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>欢笑</td>\n",
       "      <td>情绪组</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>兴奋</td>\n",
       "      <td>情绪组</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>震撼</td>\n",
       "      <td>荒诞组</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>疑惑</td>\n",
       "      <td>思考组</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>自省</td>\n",
       "      <td>思考组</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>遗憾</td>\n",
       "      <td>思考组</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>悔恨</td>\n",
       "      <td>思考组</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    tags_emotion predicted_label\n",
       "128           幽默             搞笑组\n",
       "80            温馨             情绪组\n",
       "188           开心             情绪组\n",
       "44            欢笑             情绪组\n",
       "4             兴奋             情绪组\n",
       "..           ...             ...\n",
       "66            震撼             荒诞组\n",
       "12            疑惑             思考组\n",
       "83            自省             思考组\n",
       "101           遗憾             思考组\n",
       "102           悔恨             思考组\n",
       "\n",
       "[192 rows x 2 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diction = {1:'搞笑组', 2:'情绪组', 3:'害怕组', 4:'正能量组', 5:'负能量组', 6:'烂片组', 7:'题材', 8:'主角路线', 9:'励志组', 10:'荒诞组', 11:'思考组', 12:'经典组'}\n",
    "\n",
    "df = book_test_df.sort_values('predicted_label')\n",
    "df['predicted_label'] = df['predicted_label'].map(diction)\n",
    "df = df[['tags_emotion', 'predicted_label']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Book_output.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0245 - accuracy: 0.9832 - val_loss: 2.2969 - val_accuracy: 0.2333\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0247 - accuracy: 0.9748 - val_loss: 2.2967 - val_accuracy: 0.2333\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0246 - accuracy: 0.9748 - val_loss: 2.2968 - val_accuracy: 0.2333\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0250 - accuracy: 0.9832 - val_loss: 2.2972 - val_accuracy: 0.2333\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0242 - accuracy: 0.9832 - val_loss: 2.2971 - val_accuracy: 0.2333\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0247 - accuracy: 0.9748 - val_loss: 2.2974 - val_accuracy: 0.2333\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0240 - accuracy: 0.9832 - val_loss: 2.2974 - val_accuracy: 0.2333\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0242 - accuracy: 0.9832 - val_loss: 2.2971 - val_accuracy: 0.2333\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0242 - accuracy: 0.9748 - val_loss: 2.2974 - val_accuracy: 0.2333\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0242 - accuracy: 0.9832 - val_loss: 2.2974 - val_accuracy: 0.2333\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0241 - accuracy: 0.9832 - val_loss: 2.2976 - val_accuracy: 0.2333\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0244 - accuracy: 0.9748 - val_loss: 2.2975 - val_accuracy: 0.2333\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0242 - accuracy: 0.9832 - val_loss: 2.2975 - val_accuracy: 0.2333\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0242 - accuracy: 0.9832 - val_loss: 2.2979 - val_accuracy: 0.2333\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0241 - accuracy: 0.9832 - val_loss: 2.2981 - val_accuracy: 0.2333\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0251 - accuracy: 0.9748 - val_loss: 2.2985 - val_accuracy: 0.2333\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0247 - accuracy: 0.9748 - val_loss: 2.2980 - val_accuracy: 0.2333\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0241 - accuracy: 0.9832 - val_loss: 2.2979 - val_accuracy: 0.2333\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0240 - accuracy: 0.9832 - val_loss: 2.2977 - val_accuracy: 0.2333\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0240 - accuracy: 0.9832 - val_loss: 2.2977 - val_accuracy: 0.2333\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0243 - accuracy: 0.9832 - val_loss: 2.2979 - val_accuracy: 0.2333\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0240 - accuracy: 0.9832 - val_loss: 2.2978 - val_accuracy: 0.2333\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0243 - accuracy: 0.9832 - val_loss: 2.2979 - val_accuracy: 0.2333\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0242 - accuracy: 0.9832 - val_loss: 2.2977 - val_accuracy: 0.2333\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0241 - accuracy: 0.9832 - val_loss: 2.2978 - val_accuracy: 0.2333\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0240 - accuracy: 0.9748 - val_loss: 2.2978 - val_accuracy: 0.2333\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0248 - accuracy: 0.9748 - val_loss: 2.2976 - val_accuracy: 0.2333\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0245 - accuracy: 0.9832 - val_loss: 2.2978 - val_accuracy: 0.2333\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0243 - accuracy: 0.9832 - val_loss: 2.2980 - val_accuracy: 0.2333\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0238 - accuracy: 0.9832 - val_loss: 2.2980 - val_accuracy: 0.2333\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0244 - accuracy: 0.9748 - val_loss: 2.2982 - val_accuracy: 0.2333\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0240 - accuracy: 0.9832 - val_loss: 2.2980 - val_accuracy: 0.2333\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0240 - accuracy: 0.9832 - val_loss: 2.2977 - val_accuracy: 0.2333\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0239 - accuracy: 0.9832 - val_loss: 2.2976 - val_accuracy: 0.2333\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0249 - accuracy: 0.9832 - val_loss: 2.2974 - val_accuracy: 0.2333\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0240 - accuracy: 0.9832 - val_loss: 2.2975 - val_accuracy: 0.2333\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0241 - accuracy: 0.9832 - val_loss: 2.2977 - val_accuracy: 0.2333\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0245 - accuracy: 0.9748 - val_loss: 2.2975 - val_accuracy: 0.2333\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0242 - accuracy: 0.9832 - val_loss: 2.2976 - val_accuracy: 0.2333\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0240 - accuracy: 0.9832 - val_loss: 2.2978 - val_accuracy: 0.2333\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0241 - accuracy: 0.9748 - val_loss: 2.2977 - val_accuracy: 0.2333\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0241 - accuracy: 0.9748 - val_loss: 2.2979 - val_accuracy: 0.2333\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0239 - accuracy: 0.9832 - val_loss: 2.2979 - val_accuracy: 0.2333\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0239 - accuracy: 0.9748 - val_loss: 2.2979 - val_accuracy: 0.2333\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0245 - accuracy: 0.9832 - val_loss: 2.2983 - val_accuracy: 0.2333\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0242 - accuracy: 0.9832 - val_loss: 2.2981 - val_accuracy: 0.2333\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0240 - accuracy: 0.9832 - val_loss: 2.2985 - val_accuracy: 0.2333\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0242 - accuracy: 0.9832 - val_loss: 2.2985 - val_accuracy: 0.2333\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0239 - accuracy: 0.9832 - val_loss: 2.2985 - val_accuracy: 0.2333\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0241 - accuracy: 0.9832 - val_loss: 2.2984 - val_accuracy: 0.2333\n",
      "{'loss': [0.024473704397678375, 0.024734552949666977, 0.024577610194683075, 0.02504396066069603, 0.02419225126504898, 0.024721862748265266, 0.023953057825565338, 0.024214506149291992, 0.02421603724360466, 0.024179697036743164, 0.024068545550107956, 0.024354293942451477, 0.024208785966038704, 0.024217400699853897, 0.024059288203716278, 0.025133954361081123, 0.024658078327775, 0.024074429646134377, 0.02402338944375515, 0.023952821269631386, 0.024299705401062965, 0.024009229615330696, 0.02428809553384781, 0.02419207990169525, 0.024110227823257446, 0.024012699723243713, 0.024762753397226334, 0.024484405294060707, 0.0242894496768713, 0.023796318098902702, 0.024417629465460777, 0.023974677547812462, 0.024032127112150192, 0.023940259590744972, 0.02487613447010517, 0.023984888568520546, 0.02409779652953148, 0.024491168558597565, 0.024188408628106117, 0.024026673287153244, 0.0240750964730978, 0.02409537509083748, 0.023937169462442398, 0.02390948310494423, 0.024459144100546837, 0.024216165766119957, 0.024037569761276245, 0.024245332926511765, 0.023875294253230095, 0.024132704362273216], 'accuracy': [0.9831932783126831, 0.9747899174690247, 0.9747899174690247, 0.9831932783126831, 0.9831932783126831, 0.9747899174690247, 0.9831932783126831, 0.9831932783126831, 0.9747899174690247, 0.9831932783126831, 0.9831932783126831, 0.9747899174690247, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9747899174690247, 0.9747899174690247, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9747899174690247, 0.9747899174690247, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9747899174690247, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9747899174690247, 0.9831932783126831, 0.9831932783126831, 0.9747899174690247, 0.9747899174690247, 0.9831932783126831, 0.9747899174690247, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831, 0.9831932783126831], 'val_loss': [2.296905994415283, 2.2967255115509033, 2.2967689037323, 2.2971982955932617, 2.2971158027648926, 2.2973978519439697, 2.297355890274048, 2.297121524810791, 2.297443389892578, 2.2974095344543457, 2.297579526901245, 2.297461748123169, 2.2975316047668457, 2.2978756427764893, 2.2980589866638184, 2.2984695434570312, 2.297950506210327, 2.2979001998901367, 2.297668695449829, 2.2977452278137207, 2.297863721847534, 2.2977588176727295, 2.297900438308716, 2.297663688659668, 2.2978196144104004, 2.2977871894836426, 2.297569513320923, 2.2977588176727295, 2.2980356216430664, 2.2980337142944336, 2.298175096511841, 2.2980000972747803, 2.297679901123047, 2.297624111175537, 2.2973546981811523, 2.297485113143921, 2.297656774520874, 2.297523260116577, 2.297560691833496, 2.2977774143218994, 2.2977449893951416, 2.297931432723999, 2.297905206680298, 2.2979235649108887, 2.2982592582702637, 2.29811429977417, 2.298509359359741, 2.2984509468078613, 2.2985212802886963, 2.2983591556549072], 'val_accuracy': [0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408, 0.23333333432674408]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/qiaochufeng/Desktop/c_clustering_emo/classification.ipynb 单元格 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qiaochufeng/Desktop/c_clustering_emo/classification.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qiaochufeng/Desktop/c_clustering_emo/classification.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Plot training & validation accuracy values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qiaochufeng/Desktop/c_clustering_emo/classification.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# plt.plot(history.history['accuracy'])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qiaochufeng/Desktop/c_clustering_emo/classification.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# plt.plot(history.history['val_accuracy'])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qiaochufeng/Desktop/c_clustering_emo/classification.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# # Plot training & validation loss values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qiaochufeng/Desktop/c_clustering_emo/classification.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# plt.plot(history.history['loss'])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/qiaochufeng/Desktop/c_clustering_emo/classification.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39;49mhistory[\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m-\u001b[39;49m history\u001b[39m.\u001b[39;49mhistory[\u001b[39m'\u001b[39;49m\u001b[39mval_loss\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qiaochufeng/Desktop/c_clustering_emo/classification.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mModel loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qiaochufeng/Desktop/c_clustering_emo/classification.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "# Train the model, capturing the history\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n",
    "\n",
    "# The history object now contains the records of loss and accuracy for each epoch\n",
    "print(history.history)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# # plt.legend(['Train'], loc='upper left')\n",
    "# # plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training & validation loss values\n",
    "# plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cq1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
